{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from torch.autograd.gradcheck import zero_gradients\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import json\n",
    "import numpy as np\n",
    "import requests, io\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download and load pretrained inceptionv3 model\n",
    "inceptionv3 = models.inception_v3(pretrained=True) \n",
    "inceptionv3.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload target image\n",
    "img = Image.open(\"Clean44.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set mean and std deviation\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "                transforms.Resize((299,299)),  \n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess image\n",
    "image_tensor = preprocess(img) \n",
    "# add batch dimension.  C X H X W ==> B X C X H X W\n",
    "image_tensor = image_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert tensor into a variable\n",
    "img_variable = Variable(image_tensor, requires_grad=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify image using pre-trained inceptionv3 model\n",
    "output = inceptionv3.forward(img_variable)\n",
    "#get an index(class number) of a largest element\n",
    "label_idx = torch.max(output.data,1)[1][0]   \n",
    "print(label_idx.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload JSON file containing class labels\n",
    "with open('labels.json') as f:\n",
    "    labels_json = json.load(f)\n",
    "labels = {int(idx):label for idx, label in labels_json.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign label to predicted image\n",
    "x_pred = labels[label_idx.item()]\n",
    "print(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get probability dist over classes\n",
    "output_probs = F.softmax(output, dim=1)\n",
    "x_pred_prob =  ((torch.max(output_probs.data, 1)[0][0]) * 100)\n",
    "input_prob = x_pred_prob.item()\n",
    "print (input_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change y_true to input image class  \n",
    "y_true = Variable( torch.LongTensor([285]), requires_grad=False)\n",
    "#BIM hyperparameters\n",
    "epsilon = 0.10\n",
    "num_steps = 20\n",
    "alpha = 0.025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_steps):\n",
    "  zero_gradients(img_variable)                       #flush gradients\n",
    "  output = inceptionv3.forward(img_variable)         #perform forward pass\n",
    "  loss = torch.nn.CrossEntropyLoss()\n",
    "  loss_cal = loss(output, y_true)\n",
    "  loss_cal.backward(retain_graph=True)\n",
    "  x_grad = alpha * torch.sign(img_variable.grad.data)   # as per the formula\n",
    "  adv_temp = img_variable.data + x_grad                 #add perturbation to img_variable which also contains perturbation from previous iterations\n",
    "  total_grad = adv_temp - image_tensor                  #total perturbation\n",
    "  total_grad = torch.clamp(total_grad, -epsilon, epsilon)\n",
    "  x_adv = image_tensor + total_grad                      #add total perturbation to the original image\n",
    "  img_variable.data = x_adv\n",
    "\n",
    "#final adversarial example can be accessed at- img_variable.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(x, x_adv, x_grad, epsilon, clean_pred, adv_pred, clean_prob, adv_prob):\n",
    "    \n",
    "   \n",
    "    \n",
    "    x = x.squeeze(0)     #remove batch dimension # B X C H X W ==> C X H X W\n",
    "    x = x.mul(torch.FloatTensor(std).view(3,1,1)).add(torch.FloatTensor(mean).view(3,1,1)).numpy()#reverse of normalization op- \"unnormalize\"\n",
    "    x = np.transpose( x , (1,2,0))   # C X H X W  ==>   H X W X C\n",
    "    x = np.clip(x, 0, 1)\n",
    "    \n",
    "    x_adv = x_adv.squeeze(0)\n",
    "    x_adv = x_adv.mul(torch.FloatTensor(std).view(3,1,1)).add(torch.FloatTensor(mean).view(3,1,1)).numpy()#reverse of normalization op\n",
    "    x_adv = np.transpose( x_adv , (1,2,0))   # C X H X W  ==>   H X W X C\n",
    "    x_adv = np.clip(x_adv, 0, 1)\n",
    "    \n",
    "    x_grad = x_grad.squeeze(0).numpy()\n",
    "    x_grad = np.transpose(x_grad, (1,2,0))\n",
    "    x_grad = np.clip(x_grad, 0, 1)\n",
    "    \n",
    "    figure, ax = plt.subplots(1,3, figsize=(18,8))\n",
    "    ax[0].imshow(x)\n",
    "    ax[0].set_title('Original Image', fontsize=20)\n",
    "    \n",
    "    \n",
    "    ax[1].imshow(x_grad)\n",
    "    ax[1].set_title('Perturbation', fontsize=20)\n",
    "    ax[1].set_yticklabels([])\n",
    "    ax[1].set_xticklabels([])\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "\n",
    "    \n",
    "    ax[2].imshow(x_adv)\n",
    "    ax[2].set_title('Adversarial Example', fontsize=20)\n",
    "    \n",
    "    ax[0].axis('off')\n",
    "    ax[2].axis('off')\n",
    "\n",
    "    ax[0].text(1.1,0.5, \"+{}*\".format(round(epsilon,3)), size=15, ha=\"center\", \n",
    "             transform=ax[0].transAxes)\n",
    "    \n",
    "    ax[0].text(0.5,-0.13, \"Prediction: {}\\n Probability: {}\".format(clean_pred, clean_prob), size=15, ha=\"center\", \n",
    "         transform=ax[0].transAxes)\n",
    "    \n",
    "    ax[1].text(1.1,0.5, \" = \", size=15, ha=\"center\", transform=ax[1].transAxes)\n",
    "\n",
    "    ax[2].text(0.5,-0.13, \"Prediction: {}\\n Probability: {}\".format(adv_pred, adv_prob), size=15, ha=\"center\", \n",
    "         transform=ax[2].transAxes)\n",
    "    \n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_adv = inceptionv3.forward(img_variable)\n",
    "y_add = torch.max(output_adv.data, 1)[1][0]\n",
    "x_adv_pred = labels[y_add.item()]  #classify adversarial example\n",
    "output_adv_probs = F.softmax(output_adv, dim=1)\n",
    "x_adv_pred_prob =  ((torch.max(output_adv_probs.data, 1)[0][0]) * 100)\n",
    "visualize(image_tensor, img_variable.data, total_grad, epsilon, x_pred,x_adv_pred, x_pred_prob,  x_adv_pred_prob) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
